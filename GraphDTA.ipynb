{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14142743,"sourceType":"datasetVersion","datasetId":9012992},{"sourceId":14142758,"sourceType":"datasetVersion","datasetId":9013003}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\n\nprint(\"--- Đang kiểm tra môi trường ---\")\ntorch_ver = torch.__version__.split('+')[0]\ncuda_ver = torch.version.cuda.replace('.', '')\nprint(f\"PyTorch: {torch_ver} | CUDA: {cuda_ver}\")\n\nprint(\"\\n--- Đang cài đặt RDKit ---\")\n!pip install rdkit\n\n\nprint(\"\\n--- Đang cài đặt PyTorch Geometric ---\")\ninstall_url = f\"https://data.pyg.org/whl/torch-{torch_ver}+cu{cuda_ver}.html\"\n\n!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f {install_url}\n\n!pip install torch-geometric\n\nprint(\"\\n--- Cài đặt hoàn tất! ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:04:19.420766Z","iopub.execute_input":"2025-12-13T16:04:19.421263Z","iopub.status.idle":"2025-12-13T16:04:28.904172Z","shell.execute_reply.started":"2025-12-13T16:04:19.421239Z","shell.execute_reply":"2025-12-13T16:04:28.903171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport sys\nimport json\nimport pickle\nfrom math import sqrt\nfrom random import shuffle\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# ======================================\n# RDKit & Graph utils\n# ======================================\nfrom rdkit import Chem\nfrom rdkit.Chem import MolFromSmiles\nimport networkx as nx\n\n# ======================================\n# PyTorch\n# ======================================\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, ReLU\n\n\n# ======================================\n# PyTorch Geometric\n# ======================================\nfrom torch_geometric import data as DATA\nfrom torch_geometric.data import InMemoryDataset, DataLoader\nfrom torch_geometric.nn import (\n    GCNConv,\n    GATConv,\n    GINConv,\n    global_add_pool,\n    global_mean_pool as gap,\n    global_max_pool as gmp,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:04:28.905731Z","iopub.execute_input":"2025-12-13T16:04:28.905986Z","iopub.status.idle":"2025-12-13T16:04:28.911795Z","shell.execute_reply.started":"2025-12-13T16:04:28.905961Z","shell.execute_reply":"2025-12-13T16:04:28.911053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gat_gcn.py\n\n# GCN-CNN based model\n\nclass GAT_GCN(torch.nn.Module):\n    def __init__(self, n_output=1, num_features_xd=78, num_features_xt=25,\n                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n\n        super(GAT_GCN, self).__init__()\n\n        self.n_output = n_output\n        self.conv1 = GATConv(num_features_xd, num_features_xd, heads=10)\n        self.conv2 = GCNConv(num_features_xd*10, num_features_xd*10)\n        self.fc_g1 = torch.nn.Linear(num_features_xd*10*2, 1500)\n        self.fc_g2 = torch.nn.Linear(1500, output_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n        # 1D convolution on protein sequence\n        self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)\n        self.conv_xt_1 = nn.Conv1d(in_channels=1000, out_channels=n_filters, kernel_size=8)\n        self.fc1_xt = nn.Linear(32*121, output_dim)\n\n        # combined layers\n        self.fc1 = nn.Linear(256, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.out = nn.Linear(512, self.n_output)       \n        \n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        target = data.target\n        # print('x shape = ', x.shape)\n        x = self.conv1(x, edge_index)\n        x = self.relu(x)\n        x = self.conv2(x, edge_index)\n        x = self.relu(x)\n        # apply global max pooling (gmp) and global mean pooling (gap)\n        x = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n        x = self.relu(self.fc_g1(x))\n        x = self.dropout(x)\n        x = self.fc_g2(x)\n\n        embedded_xt = self.embedding_xt(target)\n        conv_xt = self.conv_xt_1(embedded_xt)\n        # flatten\n        xt = conv_xt.view(-1, 32 * 121)\n        xt = self.fc1_xt(xt)\n\n        # concat\n        xc = torch.cat((x, xt), 1)\n        # add some dense layers\n        xc = self.fc1(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:04:28.912435Z","iopub.execute_input":"2025-12-13T16:04:28.912729Z","iopub.status.idle":"2025-12-13T16:04:28.931637Z","shell.execute_reply.started":"2025-12-13T16:04:28.912704Z","shell.execute_reply":"2025-12-13T16:04:28.931036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gat.py \n# GAT  model\nclass GATNet(torch.nn.Module):\n    def __init__(self, num_features_xd=78, n_output=1, num_features_xt=25,\n                     n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GATNet, self).__init__()\n\n        # graph layers\n        self.gcn1 = GATConv(num_features_xd, num_features_xd, heads=10, dropout=dropout)\n        self.gcn2 = GATConv(num_features_xd * 10, output_dim, dropout=dropout)\n        self.fc_g1 = nn.Linear(output_dim, output_dim)\n\n        # 1D convolution on protein sequence\n        self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)\n        self.conv_xt1 = nn.Conv1d(in_channels=1000, out_channels=n_filters, kernel_size=8)\n        self.fc_xt1 = nn.Linear(32*121, output_dim)\n\n        # combined layers\n        self.fc1 = nn.Linear(256, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        self.out = nn.Linear(256, n_output)\n\n        # activation and regularization\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        # graph input feed-forward\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.elu(self.gcn1(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.gcn2(x, edge_index)\n        x = self.relu(x)\n        x = gmp(x, batch)        \n        x = self.fc_g1(x)\n        x = self.relu(x)\n\n        # protein input feed-forward:\n        target = data.target\n        embedded_xt = self.embedding_xt(target)\n        conv_xt = self.conv_xt1(embedded_xt)\n        conv_xt = self.relu(conv_xt)\n\n        # flatten\n        xt = conv_xt.view(-1, 32 * 121)\n        xt = self.fc_xt1(xt)\n\n        # concat\n        xc = torch.cat((x, xt), 1)\n        # add some dense layers\n        xc = self.fc1(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:04:28.932720Z","iopub.execute_input":"2025-12-13T16:04:28.932886Z","iopub.status.idle":"2025-12-13T16:04:28.949421Z","shell.execute_reply.started":"2025-12-13T16:04:28.932873Z","shell.execute_reply":"2025-12-13T16:04:28.948791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gcn.py\n\n# GCN based model\nclass GCNNet(torch.nn.Module):\n    def __init__(self, n_output=1, n_filters=32, embed_dim=128,num_features_xd=78, num_features_xt=25, output_dim=128, dropout=0.2):\n\n        super(GCNNet, self).__init__()\n\n        # SMILES graph branch\n        self.n_output = n_output\n        self.conv1 = GCNConv(num_features_xd, num_features_xd)\n        self.conv2 = GCNConv(num_features_xd, num_features_xd*2)\n        self.conv3 = GCNConv(num_features_xd*2, num_features_xd * 4)\n        self.fc_g1 = torch.nn.Linear(num_features_xd*4, 1024)\n        self.fc_g2 = torch.nn.Linear(1024, output_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n        # protein sequence branch (1d conv)\n        self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)\n        self.conv_xt_1 = nn.Conv1d(in_channels=1000, out_channels=n_filters, kernel_size=8)\n        self.fc1_xt = nn.Linear(32*121, output_dim)\n\n        # combined layers\n        self.fc1 = nn.Linear(2*output_dim, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.out = nn.Linear(512, self.n_output)\n\n    def forward(self, data):\n        # get graph input\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        # get protein input\n        target = data.target\n\n        x = self.conv1(x, edge_index)\n        x = self.relu(x)\n\n        x = self.conv2(x, edge_index)\n        x = self.relu(x)\n\n        x = self.conv3(x, edge_index)\n        x = self.relu(x)\n        x = gmp(x, batch)       # global max pooling\n\n        # flatten\n        x = self.relu(self.fc_g1(x))\n        x = self.dropout(x)\n        x = self.fc_g2(x)\n        x = self.dropout(x)\n\n        # 1d conv layers\n        embedded_xt = self.embedding_xt(target)\n        conv_xt = self.conv_xt_1(embedded_xt)\n        # flatten\n        xt = conv_xt.view(-1, 32 * 121)\n        xt = self.fc1_xt(xt)\n\n        # concat\n        xc = torch.cat((x, xt), 1)\n        # add some dense layers\n        xc = self.fc1(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:04:30.291824Z","iopub.execute_input":"2025-12-13T16:04:30.292319Z","iopub.status.idle":"2025-12-13T16:04:30.301187Z","shell.execute_reply.started":"2025-12-13T16:04:30.292293Z","shell.execute_reply":"2025-12-13T16:04:30.300457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ginconv.py\n# GINConv model\nclass GINConvNet(torch.nn.Module):\n    def __init__(self, n_output=1,num_features_xd=78, num_features_xt=25,\n                 n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n\n        super(GINConvNet, self).__init__()\n\n        dim = 32\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        self.n_output = n_output\n        # convolution layers\n        nn1 = Sequential(Linear(num_features_xd, dim), ReLU(), Linear(dim, dim))\n        self.conv1 = GINConv(nn1)\n        self.bn1 = torch.nn.BatchNorm1d(dim)\n\n        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n        self.conv2 = GINConv(nn2)\n        self.bn2 = torch.nn.BatchNorm1d(dim)\n\n        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n        self.conv3 = GINConv(nn3)\n        self.bn3 = torch.nn.BatchNorm1d(dim)\n\n        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n        self.conv4 = GINConv(nn4)\n        self.bn4 = torch.nn.BatchNorm1d(dim)\n\n        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n        self.conv5 = GINConv(nn5)\n        self.bn5 = torch.nn.BatchNorm1d(dim)\n\n        self.fc1_xd = Linear(dim, output_dim)\n\n        # 1D convolution on protein sequence\n        self.embedding_xt = nn.Embedding(num_features_xt + 1, embed_dim)\n        self.conv_xt_1 = nn.Conv1d(in_channels=1000, out_channels=n_filters, kernel_size=8)\n        self.fc1_xt = nn.Linear(32*121, output_dim)\n\n        # combined layers\n        self.fc1 = nn.Linear(256, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        self.out = nn.Linear(256, self.n_output)        \n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        target = data.target\n\n        x = F.relu(self.conv1(x, edge_index))\n        x = self.bn1(x)\n        x = F.relu(self.conv2(x, edge_index))\n        x = self.bn2(x)\n        x = F.relu(self.conv3(x, edge_index))\n        x = self.bn3(x)\n        x = F.relu(self.conv4(x, edge_index))\n        x = self.bn4(x)\n        x = F.relu(self.conv5(x, edge_index))\n        x = self.bn5(x)\n        x = global_add_pool(x, batch)\n        x = F.relu(self.fc1_xd(x))\n        x = F.dropout(x, p=0.2, training=self.training)\n\n        embedded_xt = self.embedding_xt(target)\n        conv_xt = self.conv_xt_1(embedded_xt)\n        # flatten\n        xt = conv_xt.view(-1, 32 * 121)\n        xt = self.fc1_xt(xt)\n\n        # concat\n        xc = torch.cat((x, xt), 1)\n        # add some dense layers\n        xc = self.fc1(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:04:32.355820Z","iopub.execute_input":"2025-12-13T16:04:32.356099Z","iopub.status.idle":"2025-12-13T16:04:32.367132Z","shell.execute_reply.started":"2025-12-13T16:04:32.356077Z","shell.execute_reply":"2025-12-13T16:04:32.366268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# utlis.py\n\nclass TestbedDataset(InMemoryDataset):\n    def __init__(self, root='/tmp', dataset='davis', \n                 xd=None, xt=None, y=None, transform=None,\n                 pre_transform=None,smile_graph=None):\n\n        super(TestbedDataset, self).__init__(root, transform, pre_transform)\n        # benchmark dataset, default = 'davis'\n        self.dataset = dataset\n        if os.path.isfile(self.processed_paths[0]):\n            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))\n            self.data, self.slices = torch.load(\n                                                    self.processed_paths[0],\n                                                    weights_only=False\n                                                )\n        else:\n            print('Pre-processed data {} not found, doing pre-processing...'.format(self.processed_paths[0]))\n            self.process(xd, xt, y,smile_graph)\n            self.data, self.slices = self.data, self.slices = torch.load(\n                                                                    self.processed_paths[0],\n                                                                    weights_only=False\n                                                                )\n\n    @property\n    def raw_file_names(self):\n        pass\n        #return ['some_file_1', 'some_file_2', ...]\n\n    @property\n    def processed_file_names(self):\n        return [self.dataset + '.pt']\n\n    def download(self):\n        # Download to `self.raw_dir`.\n        pass\n\n    def _download(self):\n        pass\n\n    def _process(self):\n        if not os.path.exists(self.processed_dir):\n            os.makedirs(self.processed_dir)\n\n    def process(self, xd, xt, y,smile_graph):\n        assert (len(xd) == len(xt) and len(xt) == len(y)), \"The three lists must be the same length!\"\n        data_list = []\n        data_len = len(xd)\n        for i in range(data_len):\n            print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n            smiles = xd[i]\n            target = xt[i]\n            labels = y[i]\n            # convert SMILES to molecular representation using rdkit\n            c_size, features, edge_index = smile_graph[smiles]\n            # make the graph ready for PyTorch Geometrics GCN algorithms:\n            GCNData = DATA.Data(x=torch.Tensor(features),\n                                edge_index=torch.LongTensor(edge_index).transpose(1, 0),\n                                y=torch.FloatTensor([labels]))\n            GCNData.target = torch.LongTensor([target])\n            GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n            # append graph, label and target sequence to data list\n            data_list.append(GCNData)\n\n        if self.pre_filter is not None:\n            data_list = [data for data in data_list if self.pre_filter(data)]\n\n        if self.pre_transform is not None:\n            data_list = [self.pre_transform(data) for data in data_list]\n        print('Graph construction done. Saving to file.')\n        data, slices = self.collate(data_list)\n        # save preprocessed data:\n        torch.save((data, slices), self.processed_paths[0])\n\ndef rmse(y,f):\n    rmse = sqrt(((y - f)**2).mean(axis=0))\n    return rmse\ndef mse(y,f):\n    mse = ((y - f)**2).mean(axis=0)\n    return mse\ndef pearson(y,f):\n    rp = np.corrcoef(y, f)[0,1]\n    return rp\ndef spearman(y,f):\n    rs = stats.spearmanr(y, f)[0]\n    return rs\ndef ci(y,f):\n    ind = np.argsort(y)\n    y = y[ind]\n    f = f[ind]\n    i = len(y)-1\n    j = i-1\n    z = 0.0\n    S = 0.0\n    while i > 0:\n        while j >= 0:\n            if y[i] > y[j]:\n                z = z+1\n                u = f[i] - f[j]\n                if u > 0:\n                    S = S + 1\n                elif u == 0:\n                    S = S + 0.5\n            j = j - 1\n        i = i - 1\n        j = i-1\n    ci = S/z\n    return ci","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:47:45.354821Z","iopub.execute_input":"2025-12-13T15:47:45.355388Z","iopub.status.idle":"2025-12-13T15:47:45.367836Z","shell.execute_reply.started":"2025-12-13T15:47:45.355366Z","shell.execute_reply":"2025-12-13T15:47:45.367095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create_data.py\n\ndef atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n                    [atom.GetIsAromatic()])\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n    \n    c_size = mol.GetNumAtoms()\n    \n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append( feature / sum(feature) )\n\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n    g = nx.Graph(edges).to_directed()\n    edge_index = []\n    for e1, e2 in g.edges:\n        edge_index.append([e1, e2])\n        \n    return c_size, features, edge_index\n\ndef seq_cat(prot):\n    x = np.zeros(max_seq_len)\n    for i, ch in enumerate(prot[:max_seq_len]): \n        x[i] = seq_dict[ch]\n    return x  \n\nos.makedirs('data', exist_ok=True)\nos.makedirs('data/processed', exist_ok=True)\n\nall_prots = []\n\ndataset_paths = {\n    'davis': '/kaggle/input/dataset-davis/',\n    'kiba': '/kaggle/input/dataset-kiba/'\n}\ndatasets = ['kiba', 'davis'] \nfor dataset in datasets:\n    fpath = dataset_paths[dataset] \n    \n    print('convert data from DeepDTA for ', dataset)\n    print('Reading from:', fpath)\n    \n    train_fold = json.load(open(fpath + \"train_fold_setting1.txt\"))\n    train_fold = [ee for e in train_fold for ee in e ]\n    valid_fold = json.load(open(fpath + \"test_fold_setting1.txt\"))\n    ligands = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n    proteins = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n    affinity = pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1')\n    \n    drugs = []\n    prots = []\n    for d in ligands.keys():\n        lg = Chem.MolToSmiles(Chem.MolFromSmiles(ligands[d]),isomericSmiles=True)\n        drugs.append(lg)\n    for t in proteins.keys():\n        prots.append(proteins[t])\n    \n    if dataset == 'davis':\n        affinity = [-np.log10(y/1e9) for y in affinity]\n        \n    affinity = np.asarray(affinity)\n    opts = ['train','test']\n    for opt in opts:\n        rows, cols = np.where(np.isnan(affinity)==False)  \n        if opt=='train':\n            rows,cols = rows[train_fold], cols[train_fold]\n        elif opt=='test':\n            rows,cols = rows[valid_fold], cols[valid_fold]\n        \n        with open('data/' + dataset + '_' + opt + '.csv', 'w') as f:\n            f.write('compound_iso_smiles,target_sequence,affinity\\n')\n            for pair_ind in range(len(rows)):\n                ls = []\n                ls += [ drugs[rows[pair_ind]]  ]\n                ls += [ prots[cols[pair_ind]]  ]\n                ls += [ affinity[rows[pair_ind],cols[pair_ind]]  ]\n                f.write(','.join(map(str,ls)) + '\\n')        \n    \n    print('\\ndataset:', dataset)\n    print('train_fold:', len(train_fold))\n    print('test_fold:', len(valid_fold))\n    print('len(set(drugs)),len(set(prots)):', len(set(drugs)),len(set(prots)))\n    all_prots += list(set(prots))\n    \n    \nseq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\nseq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\nseq_dict_len = len(seq_dict)\nmax_seq_len = 1000\n\ncompound_iso_smiles = []\nfor dt_name in ['kiba','davis']:\n    opts = ['train','test']\n    for opt in opts:\n        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n        compound_iso_smiles += list( df['compound_iso_smiles'] )\ncompound_iso_smiles = set(compound_iso_smiles)\nsmile_graph = {}\nfor smile in compound_iso_smiles:\n    g = smile_to_graph(smile)\n    smile_graph[smile] = g\n\ndatasets = ['davis','kiba']\nfor dataset in datasets:\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    \n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n        df = pd.read_csv('data/' + dataset + '_train.csv')\n        train_drugs, train_prots,  train_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n        XT = [seq_cat(t) for t in train_prots]\n        train_drugs, train_prots,  train_Y = np.asarray(train_drugs), np.asarray(XT), np.asarray(train_Y)\n        \n        df = pd.read_csv('data/' + dataset + '_test.csv')\n        test_drugs, test_prots,  test_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n        XT = [seq_cat(t) for t in test_prots]\n        test_drugs, test_prots,  test_Y = np.asarray(test_drugs), np.asarray(XT), np.asarray(test_Y)\n\n        print('preparing ', dataset + '_train.pt in pytorch format!')\n\n        train_data = TestbedDataset(root='data', dataset=dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)\n        print('preparing ', dataset + '_test.pt in pytorch format!')\n        test_data = TestbedDataset(root='data', dataset=dataset+'_test', xd=test_drugs, xt=test_prots, y=test_Y,smile_graph=smile_graph)\n        print(processed_data_file_train, ' and ', processed_data_file_test, ' have been created')        \n    else:\n        print(processed_data_file_train, ' and ', processed_data_file_test, ' are already created')    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T15:48:04.521137Z","iopub.execute_input":"2025-12-13T15:48:04.521735Z","iopub.status.idle":"2025-12-13T15:48:15.649751Z","shell.execute_reply.started":"2025-12-13T15:48:04.521712Z","shell.execute_reply":"2025-12-13T15:48:15.648670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training.py\n\n# training function at each epoch\ndef train(model, device, train_loader, optimizer, epoch):\n    print('Training on {} samples...'.format(len(train_loader.dataset)))\n    model.train()\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n        loss.backward()\n        optimizer.step()\n        if batch_idx % LOG_INTERVAL == 0:\n            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n                                                                           batch_idx * len(data.x),\n                                                                           len(train_loader.dataset),\n                                                                           100. * batch_idx / len(train_loader),\n                                                                           loss.item()))\n\ndef predicting(model, device, loader):\n    model.eval()\n    total_preds = torch.Tensor()\n    total_labels = torch.Tensor()\n    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n    with torch.no_grad():\n        for data in loader:\n            data = data.to(device)\n            output = model(data)\n            total_preds = torch.cat((total_preds, output.cpu()), 0)\n            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n    return total_labels.numpy().flatten(),total_preds.numpy().flatten()\n\n\ndatasets = ['davis','kiba']\nMODEL_ID = 0      # 0: GIN, 1: GAT, 2: GAT_GCN, 3: GCN\nDATASET_ID = 0    # 0: davis, 1: kiba\n\nmodeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][MODEL_ID]\ndataset = datasets[DATASET_ID]\nmodel_st = modeling.__name__\n\nCUDA_ID = 0   \n\ncuda_name = f\"cuda:{CUDA_ID}\" if torch.cuda.is_available() else \"cpu\"\nprint(\"cuda_name:\", cuda_name)\n\n\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 512\nLR = 0.0005\nLOG_INTERVAL = 20\nNUM_EPOCHS = 1000\n\nprint('Learning rate: ', LR)\nprint('Epochs: ', NUM_EPOCHS)\n\n# Main program: iterate over different datasets\nfor dataset in datasets:\n    print('\\nrunning on ', model_st + '_' + dataset )\n    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n        print('please run create_data.py to prepare data in pytorch format!')\n    else:\n        train_data = TestbedDataset(root='data', dataset=dataset+'_train')\n        test_data = TestbedDataset(root='data', dataset=dataset+'_test')\n        \n        \n        train_size = int(0.8 * len(train_data))\n        valid_size = len(train_data) - train_size\n        train_data, valid_data = torch.utils.data.random_split(train_data, [train_size, valid_size])        \n        \n        \n        # make data PyTorch mini-batch processing ready\n        train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n        valid_loader = DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n        test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n\n        # training the model\n        device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n        model = modeling().to(device)\n        loss_fn = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n        best_mse = 1000\n        best_test_mse = 1000\n        best_test_ci = 0\n        best_epoch = -1\n        model_file_name = 'model_' + model_st + '_' + dataset +  '.model'\n        result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n        for epoch in range(NUM_EPOCHS):\n            train(model, device, train_loader, optimizer, epoch+1)\n            print('predicting for valid data')\n            G,P = predicting(model, device, valid_loader)\n            val = mse(G,P)\n            if val<best_mse:\n                best_mse = val\n                best_epoch = epoch+1\n                torch.save(model.state_dict(), model_file_name)\n                print('predicting for test data')\n                G,P = predicting(model, device, test_loader)\n                ret = [rmse(G,P),mse(G,P),pearson(G,P),spearman(G,P),ci(G,P)]\n                with open(result_file_name,'w') as f:\n                    f.write(','.join(map(str,ret)))\n                best_test_mse = ret[1]\n                best_test_ci = ret[-1]\n                print('rmse improved at epoch ', best_epoch, '; best_test_mse,best_test_ci:', best_test_mse,best_test_ci,model_st,dataset)\n            else:\n                print(ret[1],'No improvement since epoch ', best_epoch, '; best_test_mse,best_test_ci:', best_test_mse,best_test_ci,model_st,dataset)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}